# Python Bindings for [llama.cpp](https://github.com/ggml-org/llama.cpp) (llama-cpp-python)

## ğŸ ç°¡ä»‹

`llama-cpp-python` æ˜¯ `llama.cpp` çš„ Python bindingsï¼Œæ”¯æ´ä½¿ç”¨ Python è¼•é¬†è¼‰å…¥ã€æ¨ç†ã€ä¸¦å‘¼å« OpenAI ä»‹é¢çš„ä¼ºæœå™¨ã€‚

## åŠŸèƒ½ç‰¹è‰²

- ä½¿ç”¨ `ctypes` ç¶å®šåŸç”Ÿ C å‡½å¼åº«
- æä¾›é«˜å±¤ç´š LLM API
- æä¾› OpenAI ç›¸å®¹ä¼ºæœå™¨ (`llama-cpp-python --server`)
- æ”¯æ´ `n_gpu_layers` å•Ÿç”¨ Metal/CUDA åŠ é€Ÿ

- GitHub: [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)

## Installtion

- Python 3.8+
- C compiler
    - Linux: gcc or clang
    - Windows: Visual Studio or MinGW
    - MacOS: Xcode

To install the package, run:

```sh
pip install llama-cpp-python
```
This will also build llama.cpp from source and install it alongside this python package.

### åœ¨ macOS å®‰è£ llama-cpp-pythonï¼ˆApple Siliconï¼‰

Detailed `MacOS Metal GPU` install documentation is available at [docs/install/macos.md](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/)

#### 1. å‰ç½®æ¢ä»¶

```bash
xcode-select --install
```

#### 2. å»ºè­°ä½¿ç”¨ Miniforge å®‰è£ Conda

```bash
wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh
bash Miniforge3-MacOSX-arm64.sh
```

#### 3. å»ºç«‹ conda ç’°å¢ƒ

```bash
conda create -n llama python=3.10
conda activate llama
```

#### 4. å®‰è£ llama-cpp-python + Metal GPU åŠ é€Ÿ

Before `llama.cpp` 0.2.65
```bash
CMAKE_ARGS="-DGGML_METAL=on -DCMAKE_OSX_ARCHITECTURES=arm64 -DCMAKE_APPLE_SILICON_PROCESSOR=arm64" \
FORCE_CMAKE=1 \
pip install --upgrade --verbose --force-reinstall --no-cache-dir llama-cpp-python
```

After llama.cpp 0.2.65ï¼ˆæ¨è–¦
```bash
CMAKE_ARGS="-DLLAMA_METAL=on -DLLAMA_METAL_EMBED_LIBRARY=ON -DCMAKE_OSX_ARCHITECTURES=arm64" \
FORCE_CMAKE=1 \
pip install --upgrade --verbose --force-reinstall --no-cache-dir llama-cpp-python
```

## Example

```python
from llama_cpp import Llama

llm = Llama(
      model_path="./models/7B/llama-model.gguf",
      # n_gpu_layers=-1, # Uncomment to use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)
output = llm(
      "Q: Name the planets in the solar system? A: ", # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion
print(output)
```

By default llama-cpp-python generates completions in an OpenAI compatible format:

```sh
{
  "id": "cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  "object": "text_completion",
  "created": 1679561337,
  "model": "./models/7B/llama-model.gguf",
  "choices": [
    {
      "text": "Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.",
      "index": 0,
      "logprobs": None,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 14,
    "completion_tokens": 28,
    "total_tokens": 42
  }
}
```
