# llama.cpp

## ğŸ¦™ ä»€éº¼æ˜¯ `llama.cpp`

> Inference of Meta's LLaMA model (and others) in pure C/C++

`llama.cpp` æ˜¯ç”± Georgi Gerganov é–‹ç™¼çš„é–‹æº C/C++ å°ˆæ¡ˆï¼Œèƒ½åœ¨ CPU ä¸ŠåŸ·è¡Œ Meta çš„ LLaMA æ¨¡å‹ï¼Œç‰¹åˆ¥é©åˆåœ¨è³‡æºæœ‰é™çš„ç’°å¢ƒä¸­éƒ¨ç½² LLMã€‚

## æ ¸å¿ƒç‰¹é»

- æ”¯æ´ `GGUF` æ¨¡å‹æ ¼å¼
- æ”¯æ´å¤šç¨®å¾Œç«¯ï¼šMetalï¼ˆmacOSï¼‰ã€CUDAã€OpenBLASã€Vulkan ç­‰
- æ¨¡å‹é‡åŒ–æ”¯æ´ï¼ˆQ2_Kã€Q4_Kã€Q5_Kã€Q8_0 ç­‰ï¼‰
- å¯ç”¨æ–¼ CLI æ¨ç†èˆ‡åµŒå…¥å¼è£ç½®éƒ¨ç½²

## ç·¨è­¯èˆ‡æ¨ç†æµç¨‹

```bash
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make

# æ¨ç†
./main -m models/llama-2-7b-chat.gguf -p "Hello, world"
```

